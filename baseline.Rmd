---
title: "Baseline exploration"
output:
  html_document:
    df_print: paged
---

This is to make a baseline. Same code as here, just written in R.
https://www.kaggle.com/code/saitodevel01/gsdc2-baseline-submission
And here:
https://www.kaggle.com/code/ravishah1/gsdc2-savgol-filter-outlier-removal-with-bo

Get base station observations: https://www.ngs.noaa.gov/UFCORS/
Navigation data for GNSS constellations: https://igs.bkg.bund.de/root_ftp/IGS/BRDC/

```{r Setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, warning = FALSE, error = FALSE, message = FALSE, tidy = TRUE, fig.dim = c(10, 6), global.par = TRUE)
```

```{r}
library(tidyverse)
library(data.table)

source("baseline_functions.R")
source("helper_functions.R")
```

```{r}
blh_df = read_rds( "data/train/cleaned_to_blh.rds")
```

```{r}
# baseline score
trip_ids = unique(blh_df$tripID)
scores = purrr::map_df(trip_ids, calc_score, pred_df = blh_df)
mean(scores$score) # 4.356924
```

Plot of cleaned (green) vs ground truth (red)

```{r eval=FALSE, include=FALSE}
ggplot(pred_df) +
  geom_point(aes(x = LatitudeDegrees, y = LongitudeDegrees ), color = "red") +
  geom_point(data = gt_df, aes(x = LatitudeDegrees, y = LongitudeDegrees), color = "green", size = .5)
```

```{r map them, eval=FALSE, include=FALSE}
library(mapview)

pred_sf = sf::st_as_sf(
    x = pred_df,
    coords = c("LongitudeDegrees", "LatitudeDegrees"),
    crs = 4326,
    na.fail = FALSE
  )

gt_sf = sf::st_as_sf(
    x = gt_df,
    coords = c("LongitudeDegrees", "LatitudeDegrees"),
    crs = 4326,
    na.fail = FALSE
  )
mapviewOptions(basemaps = c('Esri.WorldImagery'))
mapview::mapview(pred_sf, col.regions = "red") + mapview(gt_sf, col.regions = "green")
```


```{r outlier}
# TODO Different radius - pandas_haversine_distance() & calc_haversine(). Could just use geosphere::distHaversine()?

# Removes NAs and takes value next to missing value - this may make the score go up slightly?
correct_outliers <- function(df, th = 2) {
  # df = df[tripID==tripID][["dist_pro"]]
  # Created to shift lat/lngs up/down and to fill last/start values with 0
  df[, c("latDeg_pre", "latDeg_pro", "lngDeg_pre", "lngDeg_pro") := .(
    shift(LatitudeDegrees, fill = 0, type = "lag"),
    shift(LatitudeDegrees, fill = 0, type = "lead"),
    shift(LongitudeDegrees, fill = 0, type = "lag"),
    shift(LongitudeDegrees, fill = 0, type = "lead")
  )]
  
  # Calculates the distance between each continuous point from point before (pre) and point after (pro)
  df[, 'dist_pre'] = calc_haversine(df[, 'latDeg_pre'], df[, 'lngDeg_pre'],
                                    df[, 'LatitudeDegrees'], df[, 'LongitudeDegrees'])
  df[, 'dist_pro'] = calc_haversine(df[, 'LatitudeDegrees'], df[, 'LongitudeDegrees'], df[, 'latDeg_pro'], df[, 'lngDeg_pro'])
  
  # Replace distance for first/last point since there was a 0
  df[1,][, 'dist_pre'] <- 0
  df[nrow(df),][, 'dist_pro'] <- 0
  
  # Looks for distances 2x sd, and take mean of values before/after point.
  pro_95 = mean(df[["dist_pro"]], na.rm = T) + sd(df[["dist_pro"]], na.rm = T) * th
  pre_95 = mean(df[["dist_pre"]], na.rm = T) + sd(df[["dist_pre"]], na.rm = T) * th
  
  # ADDED is.na(), | is.na(df$LatitudeDegrees), removed and score was same
  ind = which((df[, dist_pro] > pro_95 &
                df[, dist_pre] > pre_95) | is.na(df$LatitudeDegrees), arr.ind = TRUE)
  # ind = c(ind, which(is.na(df$LatitudeDegrees)))
  for (i in ind) {
    df[i, 'LatitudeDegrees']  = mean(df[i - 1, LatitudeDegrees],  df[i + 1, LatitudeDegrees], na.rm = T, trim = 0)
    df[i, 'LongitudeDegrees'] = mean(df[i - 1, LongitudeDegrees], df[i + 1, LongitudeDegrees], na.rm = T, trim = 0)
  }
  return(df)
}
```

```{r savgol filter}
# pracma also has savgol, but results may not match, per stackoverflow
# Smooths data
apply_savgol_filter <- function(tripID, df, wl, poly) {
  # df = df[tripID == tripID]
  df[, 'LatitudeDegrees'] = signal::sgolayfilt(df[, LatitudeDegrees], n = wl, p = poly)
  df[, 'LongitudeDegrees'] = signal::sgolayfilt(df[, LongitudeDegrees], n = wl, p = poly)
  return(df)
}
```

```{r bayesian optimization}
optimize <- function(params) {
  score_list = data.frame()

  threshold = unlist(params[1])
  window_len = unlist(params[2])
  poly_order = unlist(params[3])
  # print(threshold)
  # window_len = as.integer(window_len)
  # must be odd
  if (window_len %% 2 == 0) {
    window_len = window_len + 1
  }
  for (i in 1:length(trip_ids)) {
    trip = trip_ids[i]
    
    pred_df = blh_df[tripID == trip]
    
    # We are using BLH, not ECEF to optimize
    pred_df = correct_outliers(pred_df, threshold) |> setDT() # ??
    pred_df = apply_savgol_filter(tripID, df = pred_df, wl = window_len, poly_order)
    # print(pred_df)
    score = calc_score(trip, pred_df)
    
    score_list = bind_rows(score_list, score)
    
  }
  # score_list = bind_rows(score_list, score)
  # }
  # mean_score = mean(score_list$score)
  return (mean(score_list[["score"]]))
  # score
}
```

```{r eval=FALSE, include=FALSE}
# I don't know why this doesn't work in R
optimize <- function(threshold, window_len, poly_order) {
  # optimize <- function(params) {
  # threshold = unlist(threshold)
  # window_len = unlist(window_len)
  # poly_order = unlist(poly_order)
  # message(params[threshold])
  # params['threshold']
  # threshold |> unlist()
  # print(threshold)
  # params$threshold
  # window_len = as.integer(window_len)
  # must be odd
  # if (window_len %% 2 == 0) {
  #   window_len = window_len + 1
  # }
  return(3)
}
# https://rpubs.com/Argaadya/bayesian-optimization
space1 = tuple(threshold = c(1.5, 2.5), 
         window_len = c(7, 31),
         poly_order = c(2, 6))
search_grid <- data.frame(threshold = runif(20,1.5,2.5),
                          window_len = runif(20,7,31),
                          poly_order = runif(20,2,6))
rBayesianOptimization::BayesianOptimization(FUN = optimize, bounds = space, init_grid_dt = search_grid, n_iter = 2)
library(reticulate)
py$result$x
optimize(list(1.88, 13, 4))
py$gp_minimize(optimize, r_to_py(space1), n_cblh_dfs=10)

py_to_r(py$space)
```

```{python}
from skopt import gp_minimize
from skopt.space import Real, Integer

space = [Real(1.5, 2.5, name='threshold'), 
         Integer(7, 31, name='window_len'), 
         Integer(2, 6, name='poly_order')]

result = gp_minimize(r.optimize, space, n_calls=10) # works!
result

# Make sure wl is odd, wl be > than poly
if result.x[1]%2==0:
    result.x[1]+=1

print(f'best params:\noutlier threshhold: {result.x[0]}\nsavgol filter window length: {result.x[1]}\nsavgol filter poly order: {result.x[2]}')
result.x
result.fun
# for blh_df score 3.935 [2.3059423828286683, 9, 2]
```

```{r}
# let's apply this optimize parameters
threshold = 2.3059423828286683
window_len = 9
poly_order = 2
score_list = data.frame()

  for (i in 1:length(trip_ids)) {
    trip = trip_ids[i]
    
    pred_df = blh_df[tripID == trip]
    
    # We are using BLH, not ECEF to optimize
    pred_df = correct_outliers(pred_df, threshold) |> setDT() # ??
    pred_df = apply_savgol_filter(tripID, df = pred_df, wl = window_len, poly_order)
    # print(pred_df)
    score = calc_score(trip, pred_df)
    score_list = bind_rows(score_list, score)
  }
mean(score_list$score) # 3.934705
```








